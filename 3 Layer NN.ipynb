{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organic Neural Network\n",
    "\n",
    "Create a 3-layer neural network from scratch with python for the MNIST dataset\n",
    "\n",
    "* Layer 1: inputs\n",
    "* Layer 2: hidden layer\n",
    "* Layer 3: outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "import os\n",
    "from array import array as pyarray\n",
    "from mnist import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the MNIST dataset. There is a python package to translate the page called python-mnist. You need to download the data set as well and direct the \"MNIST()\" function to where these data sets are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mndata=MNIST('C:\\Users\\schultz\\Documents\\School\\ML Class')\n",
    "[train_images,train_labels]=mndata.load_training()\n",
    "[test_images,test_labels]=mndata.load_testing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create our neural network architecture\n",
    "\n",
    "* **Layer 1**: 1 input node for each dimension of x\n",
    "    for the mnist data, there are 784 pixels per image\n",
    "\n",
    "    The input nodes are transformed by a linear transformation before going to the hidden layer\n",
    "    $z_1 = x_iW_i + b_i$\n",
    "    \n",
    "* **Layer 2**: hidden nodes of a variable number\n",
    "    \n",
    "    The hidden nodes are transformed by an activation function within the layer\n",
    "    $a_2 = activation(z_1)$\n",
    "    \n",
    "    The outputs of the activation function are transformed by a linear transformation before going to the output layer\n",
    "    $z_2 = a_2W_i + b_i$\n",
    "    \n",
    "* **Layer 3**: 1 output node for each possible value of the output\n",
    "    for the mnist data, each picture can be 0-9 so there will be 10 output nodes\n",
    "    \n",
    "    The output nodes are transformed by an activation function \n",
    "    $a_3 = y = activation(z_2)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_3lnetwork(nn_in,nn_out,nn_hid):\n",
    "    #nn_in = number of nodes in the input layer\n",
    "    #nn_out = number of nodes in the output layer\n",
    "    #nn_hid = number of nodes in the hidden layer\n",
    "\n",
    "    #we have 2 linear transformation per hidden layer (input to hidden, hidden to output)\n",
    "    np.random.seed(0)\n",
    "    #weights of the linear transformation between the input and hidden layer is between 0 and 1\n",
    "    Wih=.001*np.random.rand(nn_in,nn_hid)\n",
    "    #bias of the linear transformation between the input and hidden layer can be any value so N(0,1)\n",
    "    Bih=np.random.randn(1,nn_hid)\n",
    "    #weights of the linear transformation between the hidden and ouptut layer is between 0 and 1\n",
    "    Who=.001*np.random.rand(nn_hid,nn_out)\n",
    "    #bias of the linear transformation between the hidden and output layer can be any value so N(0,1)\n",
    "    Bho=np.random.randn(1,nn_out)\n",
    "    \n",
    "    #return our structure that has been encoded by these weights and biases!\n",
    "    return Wih,Bih,Who,Bho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------\n",
    "---------------------Activation Functions---------------------\n",
    "--------------------------------------------------------------\n",
    "\n",
    "Multiple activation functions exist. Below we've encoded a few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_function(z):\n",
    "    #hidden layer uses sigmoid\n",
    "    return 1/(1 + np.exp(-z))  \n",
    "\n",
    "def tanh_function(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def relu_function(z):       \n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def softmax_function(z):\n",
    "    return np.exp(z)/np.sum(np.exp(z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------\n",
    "--------------Our Final Neural Network Plan-------------------\n",
    "--------------------------------------------------------------\n",
    "\n",
    "* **Layer 1**: input x\n",
    "\n",
    "    $z_1 = xW_ih + b_ih$\n",
    "    \n",
    "* **Layer 2**: activation function = relu function\n",
    "\n",
    "    $a_2 = relu(z_1)$\n",
    "    \n",
    "    $z_2 = a_2W_ho + b_ho$\n",
    "    \n",
    "* **Layer 3**: activation function = softmax\n",
    "\n",
    "    $a_3 = y = softmax(z_2)$\n",
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Forward Propregation\n",
    "\n",
    "We apply our transformations to 1 set of inputs to get 1 set of outputs from our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_prop(inputs_list,Wih,Bih,Who,Bho):\n",
    "    x=np.reshape(inputs_list,(1,len(inputs_list)))\n",
    "    hidden_inputs=x.dot(Wih)+Bih\n",
    "    hidden_outputs=relu_function(hidden_inputs)\n",
    "    \n",
    "    final_inputs=hidden_outputs.dot(Who)+Bho\n",
    "    final_outputs=softmax_function(final_inputs)\n",
    "    return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 3: Loss Function\n",
    "\n",
    "We need to compare how well our network did ($y$) to the actual target answer ($t$). This is evaluated through a Loss Function ($\\xi$). \n",
    "Multiple loss functions exist but we will be using the cross-entropy loss function:\n",
    "\n",
    "$\\xi(t,y) = -\\sum_{i=1}^n t_i log(y_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(yhat,target):\n",
    "    return - np.multiply(target,np.log(yhat)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Back propogation\n",
    "\n",
    "We need to push the error we experienced between the network output and the actual target answer, currently defined in our loss function, back up the chains to update our network so next time it will predict better\n",
    "\n",
    "How do we do this? Derivatives!!\n",
    "\n",
    "Taking the derivative of each layer, we can use the chain rule to influence the weights and biases up to the input layer\n",
    "\n",
    "For the best and most thorough explination of this, please go read his Vectorization of the backward step:\n",
    "http://peterroelants.github.io/posts/neural_network_implementation_part04/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------\n",
    "---------------Derivative of Activation Function-----------------\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "*Note: All of these derivatives assume we are using the original activation function's output and not its input. \n",
    "If using input, for example, the derivative of tanh is 1 - np.multiply(np.tanh(z),np.tanh(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deriv_sigmoid(z):\n",
    "    return np.multiply(z,(1-z))\n",
    "\n",
    "def deriv_tanh(z):\n",
    "    return 1-np.multiply(z,z)\n",
    "\n",
    "def relu_function(z):       \n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def deriv_relu(z):\n",
    "    z[z<=0]=0\n",
    "    z[z>0]=1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputs_list,targets_list,Wih,Bih,Who,Bho):\n",
    "    x=np.reshape(inputs_list,(1,len(inputs_list)))\n",
    "    t=np.reshape(targets_list,(1,len(targets_list)))\n",
    "    hidden_inputs=x.dot(Wih)+Bih\n",
    "    hidden_outputs=relu_function(hidden_inputs)\n",
    "    \n",
    "    final_inputs=hidden_outputs.dot(Who)+Bho\n",
    "    final_outputs=softmax_function(final_inputs)\n",
    "\n",
    "    loss=loss_function(final_outputs,t)\n",
    "    \n",
    "#    dL/dz2 = dL/dy * dy/dz2 = Y-T\n",
    "    dL_dz2=final_outputs-t\n",
    "#    dz2/da2 = d/da2[a2*W_ho + b_ho] = W_ho\n",
    "#    dL/da2 = dL/dz2*dz2/da2\n",
    "    dL_da2=dL_dz2.dot(Who.T)\n",
    "#    da2/dz1 = d/dz1[activation_function(z1)] = derivative(activation function(hidden_outputs))\n",
    "    der_act=deriv_relu(hidden_outputs)\n",
    "\n",
    "#    dz2/dBho = d/dBho[a2*W_ho + b_ho] = 1\n",
    "#    dL/dBho = dL/dz2*dz2/dBho = dL/dz2*1 = dL/dz2\n",
    "    dL_Bho=dL_dz2\n",
    "#    dz2/dWho = d/dWho[a2*W_ho + b_ho] = a2 = hidden_outputs\n",
    "#    dL/dWho = dL/dz2*dz2/dWho\n",
    "    dL_Who=hidden_outputs.T.dot(dL_dz2)\n",
    "    \n",
    "#    dL/dBih = d/Bih[x*W_ih + b_ih]= 1\n",
    "#    dL/dBih = dL/da2*da2/dz1 = dL/da2*da2/dz1\n",
    "    dL_Bih=np.multiply(der_act,dL_da2)\n",
    "    \n",
    "#    dL/dWih = d/Wih[x*W_ih + b_ih]= x\n",
    "#    dL/dWih = dL/da2*da2/dz1*dz1/dWih = dL_Bih*dz1/dWih\n",
    "    dL_Wih=x.T.dot(dL_Bih)\n",
    "    \n",
    "    \n",
    "    return loss,dL_Who,dL_Bho,dL_Wih,dL_Bih"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Update our Original Network Weights\n",
    "\n",
    "Now that we have by what amount our originals should be updated, we need to update this. \n",
    "\n",
    "But how much weight do we put on our 'teaching'? Having one data point shouldn't wipe out our previous calculations (especially when we're 100, 200, or 1000 data points in!)\n",
    "\n",
    "This is where the learning rate comes in. The learning rate is how much we want our one data point to influence a weight and bias  of our entire system. Typically this is around 0.1-0.0001\n",
    "\n",
    "Below is what we do for 1 training point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-235a632a6c22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtarget_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdL_Who\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdL_Bho\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdL_Wih\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdL_Bih\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mWih\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBih\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mWho\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBho\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mWih\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWih\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdL_Wih\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "lr=0.001\n",
    "target_list=np.zeros(10)\n",
    "target_list[train_labels[i]]=1\n",
    "[loss,dL_Who,dL_Bho,dL_Wih,dL_Bih]=train(train_images[i],target_list,Wih,Bih,Who,Bho)\n",
    "Wih=Wih-(lr*dL_Wih)\n",
    "Bih=Bih-(lr*dL_Bih)\n",
    "Who=Who-(lr*dL_Who)\n",
    "Bho=Bho-(lr*dL_Bho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train our network!!\n",
    "\n",
    "We will use all 2000 training points for the mnist data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "[Wih,Bih,Who,Bho]=create_3lnetwork(np.shape(train_images)[1],10,200)\n",
    "for i in range(0,np.shape(train_images)[0]):\n",
    "    target_list=np.zeros(10)\n",
    "    target_list[train_labels[i]]=1\n",
    "    [loss,dL_Who,dL_Bho,dL_Wih,dL_Bih]=train(train_images[i],target_list,Wih,Bih,Who,Bho)\n",
    "    Wih=Wih-(lr*dL_Wih)\n",
    "    Bih=Bih-(lr*dL_Bih)\n",
    "    Who=Who-(lr*dL_Who)\n",
    "    B=Bho-(lr*dL_Bho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how our trained network does!!\n",
    "\n",
    "We will use all 2000 test points for the mnist data set and get a percentage of what we correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match=0\n",
    "for i in range(0,len(test_images)):\n",
    "    [final_outputs]=forward_prop(test_images[i],Wih,Bih,Who,Bho)\n",
    "    if np.argmax(final_outputs)==test_labels[i]:\n",
    "        match=match+1\n",
    "\n",
    "print(\"at a learning rate of %f and hidden nodes of %d, %f %% tests matched\"%(lr,np.shape(Wih)[1],float(match)/len(test_images)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
